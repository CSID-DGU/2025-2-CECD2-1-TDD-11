from promptflow.core import tool
from typing import Dict, List, Tuple
import json
import re
import sys
import os
import time
from uuid import uuid4

# engine ëª¨ë“ˆ import ê²½ë¡œ ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

from engine.core import InterviewEngine
from engine.utils import HINTS, EX_HINTS, CON_HINTS, hit_any, restore_categories_state
from engine.generators import generate_first_question, generate_question_llm
import redis

# ì‹¤ì œ í•¨ìˆ˜ êµ¬í˜„
def publish_delta_change(user_id, autobiography_id, theme_id, category_id, chunk_deltas=None, material_deltas=None):
    """ì‹¤ì œ ë³€í™”ëŸ‰ì„ CategoriesPayloadë¡œ ì „ì†¡"""
    try:
        # serve ë””ë ‰í† ë¦¬ ê²½ë¡œ ì¶”ê°€
        serve_dir = os.path.join(current_dir, '..', '..', '..', '..', 'serve')
        sys.path.insert(0, serve_dir)
        from stream import publish_categories_message
        from stream.dto import ChunksPayload, MaterialsPayload, CategoriesPayload
        
        # None ê°’ ì²´í¬
        if user_id is None or autobiography_id is None:
            print("[DEBUG] Skipping publish_delta_change due to None values")
            return
            
        # í˜„ì¬ ì‹œê°„
        from datetime import datetime, timezone
        timestamp = datetime.now(timezone.utc)
        
        # ChunksPayload ìƒì„± (chunk_idë¥¼ chunkOrderë¡œ ë§¤í•‘)
        chunks = []
        if chunk_deltas:
            for chunk_data in chunk_deltas:
                chunks.append(ChunksPayload(
                    categoryId=category_id,
                    chunkOrder=chunk_data.get('chunk_id', 0),  # chunk_id â†’ chunkOrder
                    weight=chunk_data.get('weight_delta', 0),   # ë³€í™”ëŸ‰
                    timestamp=timestamp
                ))
        
        # MaterialsPayload ìƒì„± (material_idë¥¼ materialOrderë¡œ ë§¤í•‘)
        materials = []
        if material_deltas:
            for material_data in material_deltas:
                materials.append(MaterialsPayload(
                    chunkId=material_data.get('chunk_id', 0),     # ì–´ëŠ chunkì— ì†í•˜ëŠ”ì§€
                    materialOrder=material_data.get('material_id', 0), # material_id â†’ materialOrder
                    example=material_data.get('example_delta', 0),     # ë³€í™”ëŸ‰
                    similarEvent=material_data.get('similar_event_delta', 0), # ë³€í™”ëŸ‰
                    count=material_data.get('count_delta', 0),         # ë³€í™”ëŸ‰
                    principle=material_data.get('principle_delta', [0,0,0,0,0,0]), # ë³€í™”ëŸ‰ ë°°ì—´
                    timestamp=timestamp
                ))
        
        # CategoriesPayload ìƒì„± ë° ì „ì†¡
        payload = CategoriesPayload(
            autobiographyId=int(autobiography_id),
            userId=int(user_id),
            themeId=theme_id,
            categoryId=category_id,
            chunks=chunks,
            materials=materials
        )
        
        publish_categories_message(payload)
        print(f"[DEBUG] Published delta change: category={category_id}, {len(chunks)} chunks, {len(materials)} materials")
        
    except Exception as e:
        print(f"[WARN] Delta ë°œí–‰ ì‹¤íŒ¨: {e}")
        pass


# ------------------------ ê°„ë‹¨ í—¬í¼ ------------------------

def _norm(s: str) -> str:
    """ê³µë°± ì œê±° + trim í›„ ë¹„êµìš© ë¬¸ìì—´ë¡œ ì •ê·œí™”"""
    return re.sub(r"\s+", "", (s or "").strip())


def _build_materials_list(material_data: dict) -> List[str]:
    """ì¹´í…Œê³ ë¦¬-ì²­í¬-ì†Œì¬ í’€ë„¤ì„ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: 'ì¹´í…Œê³ ë¦¬ ì²­í¬ ì†Œì¬')"""
    out: List[str] = []
    for category in material_data.get("category", []):
        c = category.get("name", "")
        for chunk in category.get("chunk", []):
            ch = chunk.get("name", "")
            for material in chunk.get("material", []):
                out.append(f"{c} {ch} {material}")
    return out


def _load_mapping(mapping_path: str) -> Tuple[dict, dict]:
    """
    material_id_mapping.json ë¡œë“œ
    - ë°˜í™˜1: ì›ë³¸ ë§¤í•‘ { "ì¹´í…Œê³ ë¦¬ ì²­í¬ ì†Œì¬": [cat,chunk,mat], ... }
    - ë°˜í™˜2: ê³µë°± ì œê±° ì¸ë±ìŠ¤ { "ì¹´í…Œê³ ë¦¬ì²­í¬ì†Œì¬": "ì¹´í…Œê³ ë¦¬ ì²­í¬ ì†Œì¬", ... }
    """
    try:
        with open(mapping_path, "r", encoding="utf-8") as f:
            mapping = json.load(f)
        norm_index = {_norm(k): k for k in mapping.keys()}
        return mapping, norm_index
    except Exception as e:
        print(f"[ERROR] material_id_mapping.json ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}, {}


def _call_llm_map_flow(flow_path: str, answer_text: str, materials_list: List[str], current_material: str) -> List[dict]:
    """
    LLM í”Œë¡œìš° í˜¸ì¶œ â†’ ì§€ì •ëœ ë‹¨ í•˜ë‚˜ì˜ í¬ë§·ë§Œ ê°€ì •:
    [
      {"material":"ì¹´í…Œê³ ë¦¬ ì²­í¬ ì†Œì¬ëª…",
       "axes":{"principle":[0,1,1,0,1,0],"example":1,"similar_event":1}}
    ]
    ì‹¤íŒ¨/ë¹„ì •ìƒ ì‹œ ë‹¨ì¼ í´ë°±: ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    """
    if not os.path.exists(flow_path):
        print(f"[WARN] flow íŒŒì¼ ì—†ìŒ: {flow_path}")
        return []

    try:
        from promptflow import load_flow
        flow = load_flow(flow_path)
        res = flow(
            answer_text=answer_text,
            materials_list=materials_list,
            current_material=current_material
        )
        items = res.get("analysis_result", [])
        # í˜¹ì‹œ ë¬¸ìì—´ì´ë¼ë©´ í•œ ë²ˆë§Œ json.loads ì‹œë„
        if isinstance(items, str):
            try:
                items = json.loads(items)
            except Exception:
                return []
        if not isinstance(items, list):
            return []
        return items
    except Exception as e:
        print(f"[ERROR] LLM í”Œë¡œìš° í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return []  # í´ë°±: ë¹ˆ ê²°ê³¼


# AI cat_numì„ DBì˜ theme_id, category_orderë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def convert_cat_num_to_db_mapping(cat_num):
    """AIì˜ cat_num(0-based)ì„ DBì˜ (theme_id, category_order)ë¡œ ë³€í™˜"""
    # material.jsonì˜ category ìˆœì„œì™€ DBì˜ theme-category ë§¤í•‘
    # AI material.json: 0=ë¶€ëª¨, 1=ì¡°ë¶€ëª¨, 2=í˜•ì œ, 3=ìë…€/ìœ¡ì•„, 4=ì¹œì²™, 5=ê°€ì¡±ì‚¬ê±´, 6=ì—°ì¸, 7=ê²°í˜¼, 8=ë°°ìš°ì, 9=ìë…€/ìœ¡ì•„, 10=ì¹œêµ¬, 11=ì§ì¥, 12=ì§„ë¡œ, 13=ë¬¸ì œí•´ê²°, 14=ìƒì• ì£¼ê¸°, 15=ì„±ê²©, 16=ì·¨ë¯¸, 17=ë°˜ë ¤ë™ë¬¼, 18=ì² í•™, 19=ì£¼ê±°ì§€, 20=ìƒí™œ, 21=ê¸ˆì „
    
    # DB ë§¤í•‘ (theme_id, category_order)
    mapping = [
        (1, 1),  # 0: ë¶€ëª¨
        (1, 2),  # 1: ì¡°ë¶€ëª¨  
        (1, 3),  # 2: í˜•ì œ
        (1, 4),  # 3: ìë…€/ìœ¡ì•„
        (1, 5),  # 4: ì¹œì²™
        (1, 6),  # 5: ê°€ì¡± ì‚¬ê±´
        (2, 1),  # 6: ì—°ì¸
        (2, 2),  # 7: ê²°í˜¼
        (2, 3),  # 8: ë°°ìš°ì
        (3, 1),  # 9: ìë…€/ìœ¡ì•„ (ë‹¤ë¥¸ theme)
        (6, 1),  # 10: ì¹œêµ¬
        (7, 1),  # 11: ì§ì¥
        (7, 2),  # 12: ì§„ë¡œ
        (7, 3),  # 13: ë¬¸ì œí•´ê²°(ê³¼ì •)
        (8, 1),  # 14: ìƒì• ì£¼ê¸°
        (5, 1),  # 15: ì„±ê²©
        (11, 1), # 16: ì·¨ë¯¸
        (12, 1), # 17: ë°˜ë ¤ë™ë¬¼
        (9, 2),  # 18: ì² í•™
        (4, 1),  # 19: ì£¼ê±°ì§€
        (3, 3),  # 20: ìƒí™œ
        (10, 1), # 21: ê¸ˆì „
    ]
    
    if 0 <= cat_num < len(mapping):
        return mapping[cat_num]
    else:
        return (1, 1)  # ê¸°ë³¸ê°’

@tool
def interview_engine(sessionId: str, answer_text: str, user_id: int, autobiography_id: int) -> Dict:
    """ì¸í„°ë·° ì—”ì§„ - Redisì—ì„œ ì„¸ì…˜ ë¡œë“œí•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ ìƒì„±"""

    # Redisì—ì„œ ì„¸ì…˜ ë¡œë“œ
    import redis
    import os
    
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ Redis ì„¤ì • ì½ê¸°
    redis_host = os.getenv('REDIS_HOST')
    redis_port = int(os.getenv('REDIS_PORT'))
    redis_client = redis.Redis(host=redis_host, port=redis_port, db=0, decode_responses=True)
    session_key = f"session:{sessionId}"
    session_data_raw = redis_client.get(session_key)
    session_data = json.loads(session_data_raw) if session_data_raw and isinstance(session_data_raw, str) else None
    print(f"[DEBUG] Session loaded")

    # ì²« ì§ˆë¬¸ ë¶„ê¸°
    if not session_data or not session_data.get("last_question"):
        preferred_categories = session_data.get("metrics", {}).get("preferred_categories", []) if session_data else []

        material_json_path = os.path.join(os.path.dirname(__file__), "data", "material.json")
        with open(material_json_path, 'r', encoding='utf-8') as f:
            material_data = json.load(f)

        categories = InterviewEngine.build_categories_from_category_json(material_data)
        engine = InterviewEngine(categories)

        metrics = {"preferred_categories": preferred_categories}
        result = generate_first_question(engine, metrics)
        result["last_answer_materials_id"] = []  # ì²« ì§ˆë¬¸ì´ë¯€ë¡œ ë¹„ì›€
        return result

    # ì´í›„ ì§ˆë¬¸ ìƒì„± ì¤€ë¹„
    question = session_data.get("last_question", {})
    metrics = session_data.get("metrics", {})

    # material.json ë¡œë“œ ë° ì—”ì§„ ì´ˆê¸°í™”
    material_json_path = os.path.join(os.path.dirname(__file__), "data", "material.json")
    try:
        with open(material_json_path, 'r', encoding='utf-8') as f:
            material_data = json.load(f)

        categories = InterviewEngine.build_categories_from_category_json(material_data)

        # ì´ì „ ë©”íŠ¸ë¦­ì´ ìˆìœ¼ë©´ ìƒíƒœ ë³µì›
        if metrics.get("categories"):
            restore_categories_state(categories, metrics["categories"])

        engine = InterviewEngine(categories)

        # ìƒíƒœ ë³µì›
        engine_state = metrics.get("engine_state", {})
        engine.state.last_material_id = engine_state.get("last_material_id")
        engine.state.last_material_streak = engine_state.get("last_material_streak", 0)
        engine.theme_initialized = engine_state.get("theme_initialized", False)

    except Exception as e:
        print(f"[ERROR] ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return {"next_question": None, "last_answer_materials_id": []}

    # ë‹µë³€ ë¶„ì„
    current_material = question.get("material", "") if question else ""
    is_first_question = not answer_text or not current_material

    matched_materials: List[str] = []
    axes_analysis_by_material: Dict[str, dict] = {}
    mapped_ids: List[List[int]] = []

    if not is_first_question:
        # 6W ì¶• ê°ì§€(íœ´ë¦¬ìŠ¤í‹±, LLM ë°˜í™˜ì— ê°’ ì—†ì„ ë•Œ ë³´ì¡°ë¡œ ì‚¬ìš©)
        axes_evidence = {k: hit_any(answer_text, HINTS[k]) for k in HINTS.keys()}
        ex_flag = 1 if hit_any(answer_text, EX_HINTS) else 0
        con_flag = 1 if hit_any(answer_text, CON_HINTS) else 0
        if not con_flag and len(answer_text or "") >= 80:
            con_flag = 1

        # ---------- ê°„ê²°í•´ì§„ LLM ê¸°ë°˜ ì†Œì¬ ë§¤í•‘ ----------
        # ìƒëŒ€ ê²½ë¡œë¡œ flows ë””ë ‰í† ë¦¬ ì°¾ê¸°
        flows_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(current_dir))))
        map_flow_path = os.path.join(flows_dir, "flows", "interviews", "standard", "map_answer_to_materials", "flow.dag.yaml")
        materials_list = _build_materials_list(material_data)

        # ë§¤í•‘ íŒŒì¼ ë¡œë“œ
        mapping_path = os.path.join(os.path.dirname(__file__), "data", "material_id_mapping.json")
        material_mapping, norm_index = _load_mapping(mapping_path)

        llm_items = _call_llm_map_flow(map_flow_path, answer_text, materials_list, current_material)

        # ì§€ì • í¬ë§· ê·¸ëŒ€ë¡œ ê°€ì •: [{"material": "...", "axes": {...}}, ...]
        for item in llm_items:
            if not isinstance(item, dict):
                continue
            name = item.get("material")
            if not name:
                continue

            # 1) ì •í™• ë§¤ì¹­ â†’ 2) ê³µë°± ì œê±° í›„ ë§¤ì¹­
            key = name if name in material_mapping else norm_index.get(_norm(name))
            if not key:
                continue

            matched_materials.append(key)
            axes_analysis_by_material[key] = item.get("axes", {})

        print(f"[INFO] LLM ë¶„ì„ ì™„ë£Œ: {len(matched_materials)}ê°œ ì†Œì¬ ë§¤ì¹­")

        # ì†Œì¬ ID ë§¤í•‘
        for material_name in matched_materials:
            mid = material_mapping.get(material_name)
            if isinstance(mid, list) and len(mid) == 3:
                mapped_ids.append(mid)

        # LLMì— ì¶• ì •ë³´ê°€ ì—†ì„ ë•Œ íœ´ë¦¬ìŠ¤í‹± ë°˜ì˜
        if mapped_ids:
            for i, material_id in enumerate(mapped_ids):
                cat_num, chunk_num, mat_num = material_id
                material = engine._get_material(cat_num, chunk_num, mat_num)
                if not material:
                    continue

                axes = axes_analysis_by_material.get(matched_materials[i], {})
                axes_data = axes.get("axes", {}) if isinstance(axes, dict) else {}

                # ========== PRINCIPLE ì¦ê°€ ë¶€ë¶„ (6W ì¶•) ==========
                # principle (6W) - ê° ì¶•ë³„ë¡œ 0ì—ì„œ 1ë¡œ ë³€ê²½ë  ë•Œ ì¦ê°€
                if isinstance(axes_data.get("principle"), list) and len(axes_data["principle"]) == 6:
                    principle_delta = [0,0,0,0,0,0]
                    for j, detected in enumerate(axes_data["principle"]):
                        if detected == 1 and material.principle[j] == 0:  # 0â†’1 ë³€í™”ë§Œ ê°ì§€
                            material.principle[j] = 1  # â˜… PRINCIPLE ì¦ê°€ ì§€ì 
                            principle_delta[j] = 1
                else:
                    # íœ´ë¦¬ìŠ¤í‹± ë³´ì¡° - LLMì´ ì¶• ì •ë³´ë¥¼ ì œê³µí•˜ì§€ ì•Šì„ ë•Œ
                    principle_delta = [0,0,0,0,0,0]
                    for j, detected in enumerate(axes_evidence.values()):
                        if detected and j < 6 and material.principle[j] == 0:  # 0â†’1 ë³€í™”ë§Œ ê°ì§€
                            material.principle[j] = 1  # â˜… PRINCIPLE ì¦ê°€ ì§€ì  (íœ´ë¦¬ìŠ¤í‹±)
                            principle_delta[j] = 1

                # ========== EXAMPLE ì¦ê°€ ë¶€ë¶„ ==========
                example_delta = 0
                if (axes_data.get("example") == 1 or ex_flag) and material.example == 0:  # 0â†’1 ë³€í™”ë§Œ ê°ì§€
                    material.example = 1  # â˜… EXAMPLE ì¦ê°€ ì§€ì 
                    example_delta = 1
                    
                # ========== SIMILAR_EVENT ì¦ê°€ ë¶€ë¶„ ==========
                similar_event_delta = 0
                if (axes_data.get("similar_event") == 1 or con_flag) and material.similar_event == 0:  # 0â†’1 ë³€í™”ë§Œ ê°ì§€
                    material.similar_event = 1  # â˜… SIMILAR_EVENT ì¦ê°€ ì§€ì 
                    similar_event_delta = 1

                # material ë³€ê²½ì‚¬í•­ ë°œí–‰
                if any(principle_delta) or example_delta or similar_event_delta:
                    print(f"[DEBUG] Material changes detected: principle_delta={principle_delta}, example_delta={example_delta}, similar_event_delta={similar_event_delta}")
                    
                    # material ë³€í™”ëŸ‰ ë°ì´í„° êµ¬ì„±
                    material_deltas = [{
                        'chunk_id': chunk_num,
                        'material_id': material.order,
                        'example_delta': example_delta,
                        'similar_event_delta': similar_event_delta,
                        'count_delta': 0,
                        'principle_delta': principle_delta
                    }]
                    
                    # AI cat_numì„ DB ë§¤í•‘ìœ¼ë¡œ ë³€í™˜
                    theme_id, category_order = convert_cat_num_to_db_mapping(cat_num)
                    
                    print(f"[DEBUG] publish_delta_change params: user_id={user_id}, autobiography_id={autobiography_id}, theme_id={theme_id}, category_order={category_order}")
                    
                    publish_delta_change(
                        user_id=user_id,
                        autobiography_id=autobiography_id, 
                        theme_id=theme_id,
                        category_id=category_order,  # DBì˜ category order ì‚¬ìš©
                        material_deltas=material_deltas
                    )

                # ========== CHUNK WEIGHT ì¦ê°€ ë¶€ë¶„ ==========
                category = engine.categories[cat_num]
                old_weight = category.chunk_weight.get(chunk_num, 0)
                category.chunk_weight[chunk_num] = old_weight + 1  # â˜… CHUNK WEIGHT ì¦ê°€ ì§€ì  (+1ì”© ëˆ„ì )
                
                # chunk weight ì¦ê°€ ë°œí–‰
                # chunk weight ë³€í™”ëŸ‰ ë°ì´í„° êµ¬ì„±
                chunk_deltas = [{
                    'chunk_id': chunk_num,
                    'weight_delta': 1  # weight ì¦ê°€
                }]
                
                # AI cat_numì„ DB ë§¤í•‘ìœ¼ë¡œ ë³€í™˜
                theme_id, category_order = convert_cat_num_to_db_mapping(cat_num)
                
                publish_delta_change(
                    user_id=user_id,
                    autobiography_id=autobiography_id,
                    theme_id=theme_id,
                    category_id=category_order,  # DBì˜ category order ì‚¬ìš©
                    chunk_deltas=chunk_deltas
                )
                
                # ========== MATERIAL COUNT ì¦ê°€ ë¶€ë¶„ ==========
                material.mark_filled_if_ready()  # â˜… ì´ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ MATERIAL COUNTê°€ 0â†’1ë¡œ ë³€ê²½ë¨

        same_material = (current_material in matched_materials) if current_material else False
        print(f"\nğŸ” [ì†Œì¬ ë§¤ì¹­] {current_material} â†’ {matched_materials} (ë™ì¼:{same_material})")
        if axes_analysis_by_material:
            print("ğŸ“‹ [ì¶• ë¶„ì„ ê²°ê³¼]")
            for k, v in axes_analysis_by_material.items():
                print(f"  - {k}: {v}")
    # ------------------ ë‹¤ìŒ ì§ˆë¬¸ ìƒì„± ------------------

    try:
        material_id = engine.select_material()
        cat_num, chunk_num, mat_num = material_id

        material = engine._get_material(cat_num, chunk_num, mat_num)
        if not material:
            return {"next_question": None, "last_answer_materials_id": []}

        material_id, target = engine.select_question_in_material(material_id)
        if not target:
            return {"next_question": None, "last_answer_materials_id": []}

        # í”„ë¡¬í”„íŠ¸ìš© íƒ€ì… ë³€í™˜
        type_mapping = {
            "w1": "when", "w2": "how", "w3": "who",
            "w4": "what", "w5": "where", "w6": "why",
            "ex": "ex", "con": "con"
        }
        prompt_type = type_mapping.get(target, target)

        # ë™ì¼ ì†Œì¬ë©´ ì§ì „ ë‹µë³€ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©
        context_answer = None
        if not is_first_question:
            same_material = (current_material == material.name)
            if same_material:
                context_answer = answer_text

        category = engine.categories[cat_num]
        chunk = category.chunks[chunk_num]
        full_material_name = f"{category.category_name} {chunk.chunk_name} {material.name}"

        question_text = generate_question_llm(full_material_name, prompt_type, context_answer)

        # streak ì—…ë°ì´íŠ¸
        if engine.state.last_material_id == material_id:
            engine.state.last_material_streak += 1
        else:
            engine.state.last_material_id = material_id
            engine.state.last_material_streak = 1

        next_question = {
            "id": f"q-{uuid4().hex[:8]}",
            "material": material.name,
            "type": target,
            "text": question_text,
            "material_id": material_id
        }

        # Redisì— ì—…ë°ì´íŠ¸ëœ ìƒíƒœ ì €ì¥ (ë°°ì—´ êµ¬ì¡° ì§ë ¬í™”)
        def serialize_categories(categories):
            result = []
            for k, v in categories.items():
                # í™œì„± chunkë§Œ í¬í•¨ (chunk_weight > 0)
                active_chunks = {ck: cv for ck, cv in v.chunks.items() if v.chunk_weight.get(ck, 0) > 0}
                if not active_chunks:
                    continue

                chunks = []
                for ck, cv in active_chunks.items():
                    # í™œì„± ì†Œì¬ë§Œ í¬í•¨
                    materials = []
                    for mk, mv in cv.materials.items():
                        if any(mv.principle) or mv.example or mv.similar_event or mv.count > 0:
                            materials.append({
                                "order": mv.order,
                                "name": mv.name,
                                "principle": mv.principle,
                                "example": mv.example,
                                "similar_event": mv.similar_event,
                                "count": mv.count
                            })
                    if materials:
                        chunks.append({
                            "chunk_num": cv.chunk_num,
                            "chunk_name": cv.chunk_name,
                            "materials": materials
                        })

                if chunks:
                    active_weights = {str(ck): w for ck, w in v.chunk_weight.items() if w > 0}
                    result.append({
                        "category_num": v.category_num,
                        "category_name": v.category_name,
                        "chunks": chunks,
                        "chunk_weight": active_weights
                    })
            return result

        # ì´ì „ ìƒíƒœ ì €ì¥
        previous_categories = metrics.get("categories", [])
        
        updated_metrics = {
            "session_id": sessionId,
            "categories": serialize_categories(engine.categories),
            "engine_state": {
                "last_material_id": list(engine.state.last_material_id) if engine.state.last_material_id else [],
                "last_material_streak": engine.state.last_material_streak,
                "epsilon": engine.state.epsilon
            },
            "asked_total": metrics.get("asked_total", 0) + 1,
            "policy_version": "v2.0.0"
        }
        
        # Delta ê³„ì‚° ë° ë°œí–‰
        try:
            from datetime import datetime, timezone
            serve_dir = os.path.join(current_dir, '..', '..', '..', '..', 'serve')
            sys.path.insert(0, serve_dir)
            from stream import publish_persistence_message
            from stream.dto import ChunksPayload, MaterialsPayload, CategoriesPayload
            
            now = datetime.now(timezone.utc)
            prev_cats = {c["category_num"]: c for c in previous_categories}
            
            for curr_cat in updated_metrics["categories"]:
                cat_num = curr_cat["category_num"]
                prev_cat = prev_cats.get(cat_num, {})
                prev_chunks = {c["chunk_num"]: c for c in prev_cat.get("chunks", [])}
                chunks_deltas = []
                materials_deltas = []
                
                for curr_chunk in curr_cat["chunks"]:
                    chunk_num = curr_chunk["chunk_num"]
                    prev_chunk = prev_chunks.get(chunk_num, {})
                    
                    # chunk weight ë³€í™”
                    prev_weight = prev_chunk.get("chunk_weight", {}).get(str(chunk_num), 0) if prev_chunk else 0
                    curr_weight = curr_cat["chunk_weight"].get(str(chunk_num), 0)
                    
                    if curr_weight > prev_weight:
                        chunks_deltas.append(ChunksPayload(
                            categoryId=cat_num, chunkOrder=chunk_num,
                            weight=curr_weight - prev_weight, timestamp=now
                        ))
                    
                    # material ë³€í™”
                    prev_materials = {m["order"]: m for m in prev_chunk.get("materials", [])}
                    for curr_mat in curr_chunk["materials"]:
                        mat_order = curr_mat["order"]
                        prev_mat = prev_materials.get(mat_order, {})
                        
                        principle_delta = [curr_mat["principle"][i] - prev_mat.get("principle", [0,0,0,0,0,0])[i] for i in range(6)]
                        example_delta = curr_mat["example"] - prev_mat.get("example", 0)
                        similar_event_delta = curr_mat["similar_event"] - prev_mat.get("similar_event", 0)
                        count_delta = curr_mat["count"] - prev_mat.get("count", 0)
                        
                        if any(principle_delta) or example_delta or similar_event_delta or count_delta:
                            materials_deltas.append(MaterialsPayload(
                                chunkId=chunk_num, materialOrder=mat_order,
                                example=example_delta, similarEvent=similar_event_delta,
                                count=count_delta, principle=principle_delta, timestamp=now
                            ))
                
                if chunks_deltas or materials_deltas:
                    publish_persistence_message(CategoriesPayload(
                        autobiographyId=str(session_data.get("metrics", {}).get("autobiography_id")),
                        userId=str(session_data.get("metrics", {}).get("user_id")),
                        categoryId=cat_num, chunks=chunks_deltas, materials=materials_deltas
                    ))
        except Exception as e:
            print(f"[WARN] Delta ë°œí–‰ ì‹¤íŒ¨: {e}")

        session_update = {
            "metrics": updated_metrics,
            "last_question": next_question,
            "updated_at": time.time()
        }
        redis_client.setex(session_key, 3600, json.dumps(session_update))

        print(f"\nğŸ¯ [ì§ˆë¬¸ ìƒì„±] {category.category_name}-{chunk.chunk_name}-{material.name} ({target})")
        print("=" * 50)

        last_answer_materials_id = mapped_ids if mapped_ids else []
        return {"next_question": next_question, "last_answer_materials_id": last_answer_materials_id}

    except Exception as e:
        print(f"[ERROR] ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return {"next_question": None, "last_answer_materials_id": []}
